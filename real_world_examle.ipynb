{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4e8751",
   "metadata": {},
   "source": [
    "Here's a comprehensive deep dive into EDA techniques in Python with detailed explanations, practical examples, and best practices:\n",
    "\n",
    "### 1. Data Inspection (The Foundation of EDA)\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Comprehensive inspection workflow\n",
    "def full_inspection(df):\n",
    "    print(\"=== SHAPE ===\")\n",
    "    print(f\"{df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    print(\"\\n=== DATA TYPES ===\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n=== SAMPLE DATA ===\")\n",
    "    display(df.sample(5, random_state=42))\n",
    "    \n",
    "    print(\"\\n=== MISSING VALUES ===\")\n",
    "    print(df.isna().sum().sort_values(ascending=False))\n",
    "    \n",
    "    print(\"\\n=== UNIQUE COUNTS ===\")\n",
    "    print(df.nunique())\n",
    "    \n",
    "    print(\"\\n=== DESCRIPTIVE STATS ===\")\n",
    "    display(df.describe(include='all', datetime_is_numeric=True))\n",
    "\n",
    "# Advanced techniques:\n",
    "# - Memory optimization\n",
    "df = df.astype({'category_col': 'category'})  # Saves 80%+ memory for strings\n",
    "print(f\"Memory reduced from {df.memory_usage().sum()/1e6:.2f}MB to {df.memory_usage().sum()/1e6:.2f}MB\")\n",
    "\n",
    "# - Schema validation\n",
    "expected_cols = ['id', 'date', 'value']\n",
    "assert set(df.columns) == set(expected_cols), \"Schema mismatch!\"\n",
    "```\n",
    "\n",
    "### 2. Missing Value Handling (Advanced Strategies)\n",
    "```python\n",
    "# Create missing value matrix\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "\n",
    "# Advanced imputation strategies\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Multivariate imputation\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Categorical imputation\n",
    "df['category_col'] = df.groupby('group_col')['category_col'].apply(\n",
    "    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Unknown'\n",
    ")\n",
    "\n",
    "# Missing value flags\n",
    "df['is_missing'] = df['important_col'].isna().astype(int)\n",
    "\n",
    "# Advanced visualization\n",
    "msno.heatmap(df, figsize=(10,6))\n",
    "plt.title('Missing Value Correlation Heatmap')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 3. Summary Statistics (Beyond describe())\n",
    "```python\n",
    "# Comprehensive statistical profile\n",
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"EDA Report\")\n",
    "profile.to_file(\"eda_report.html\")\n",
    "\n",
    "# Advanced statistical tests\n",
    "from scipy import stats\n",
    "\n",
    "# Normality tests\n",
    "for col in numeric_cols:\n",
    "    stat, p = stats.shapiro(df[col].dropna())\n",
    "    print(f\"{col}: {'Normal' if p > 0.05 else 'Non-normal'} (p={p:.4f})\")\n",
    "\n",
    "# Skewness and kurtosis analysis\n",
    "skewness = df[numeric_cols].skew()\n",
    "kurtosis = df[numeric_cols].kurtosis()\n",
    "\n",
    "# Quantile analysis\n",
    "quantiles = df[numeric_cols].quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "\n",
    "# Outlier-resistant statistics\n",
    "from scipy.stats import trim_mean\n",
    "print(f\"10% Trimmed Mean: {trim_mean(df['value'], proportiontocut=0.1):.2f}\")\n",
    "```\n",
    "\n",
    "### 4. Data Type Validation (Professional Approach)\n",
    "```python\n",
    "# Schema enforcement decorator\n",
    "def enforce_types(expected_dtypes):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            df = func(*args, **kwargs)\n",
    "            for col, dtype in expected_dtypes.items():\n",
    "                if col in df.columns:\n",
    "                    try:\n",
    "                        df[col] = df[col].astype(dtype)\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Type conversion failed for {col}: {str(e)}\")\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            return df\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Date parsing with multiple formats\n",
    "def parse_dates(df):\n",
    "    date_formats = ['%Y-%m-%d', '%m/%d/%Y', '%d-%b-%y', '%Y%m%d']\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'], format=fmt)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "# Advanced type inference\n",
    "from pandas.api.types import infer_dtype\n",
    "print(infer_dtype(df['mystery_col']))\n",
    "```\n",
    "\n",
    "### 5. Outlier Detection (Production-Grade)\n",
    "```python\n",
    "# Comprehensive outlier detection function\n",
    "def detect_outliers(df, method='iqr', threshold=1.5):\n",
    "    outliers = pd.DataFrame()\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        Q1 = df.quantile(0.25)\n",
    "        Q3 = df.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((df < (Q1 - threshold * IQR)) | (df > (Q3 + threshold * IQR))\n",
    "    \n",
    "    elif method == 'zscore':\n",
    "        from scipy import stats\n",
    "        outliers = np.abs(stats.zscore(df)) > threshold\n",
    "    \n",
    "    elif method == 'mahalanobis':\n",
    "        from scipy.spatial import distance\n",
    "        cov = df.cov()\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        mean = df.mean()\n",
    "        mahalanobis_dist = df.apply(lambda x: distance.mahalanobis(x, mean, inv_cov), axis=1)\n",
    "        outliers = mahalanobis_dist > threshold\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Visual outlier analysis\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df[numeric_cols], orient='h', whis=1.5)\n",
    "plt.title('Multi-feature Outlier Detection')\n",
    "plt.show()\n",
    "\n",
    "# Outlier treatment pipeline\n",
    "def treat_outliers(df, strategy='clip'):\n",
    "    if strategy == 'clip':\n",
    "        lower = df.quantile(0.05)\n",
    "        upper = df.quantile(0.95)\n",
    "        return df.clip(lower, upper, axis=1)\n",
    "    \n",
    "    elif strategy == 'transform':\n",
    "        return np.log1p(df)\n",
    "    \n",
    "    elif strategy == 'impute':\n",
    "        from sklearn.neighbors import LocalOutlierFactor\n",
    "        lof = LocalOutlierFactor()\n",
    "        outliers = lof.fit_predict(df)\n",
    "        df[outliers == -1] = np.nan\n",
    "        return df.fillna(df.median())\n",
    "```\n",
    "\n",
    "### 6. Distribution Visualization (Advanced Techniques)\n",
    "```python\n",
    "# Multi-plot distribution analysis\n",
    "def plot_distributions(df, cols, hue=None):\n",
    "    plt.figure(figsize=(16, len(cols)*4))\n",
    "    \n",
    "    for i, col in enumerate(cols, 1):\n",
    "        plt.subplot(len(cols), 2, 2*i-1)\n",
    "        if df[col].dtype == 'object':\n",
    "            sns.countplot(data=df, x=col, hue=hue)\n",
    "            plt.xticks(rotation=45)\n",
    "        else:\n",
    "            sns.histplot(data=df, x=col, hue=hue, kde=True, bins=30)\n",
    "            plt.axvline(df[col].mean(), color='r', linestyle='--')\n",
    "            plt.axvline(df[col].median(), color='g', linestyle='-')\n",
    "        \n",
    "        plt.subplot(len(cols), 2, 2*i)\n",
    "        if df[col].nunique() > 10:\n",
    "            sns.boxplot(data=df, x=hue, y=col)\n",
    "        else:\n",
    "            sns.violinplot(data=df, x=col, y=hue, orient='h')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# QQ plots for normality check\n",
    "import statsmodels.api as sm\n",
    "sm.qqplot(df['value'], line='s')\n",
    "plt.title('Q-Q Plot for Normality Check')\n",
    "plt.show()\n",
    "\n",
    "# Cumulative distribution function\n",
    "plt.figure(figsize=(10,6))\n",
    "for group in df['group_col'].unique():\n",
    "    sns.ecdfplot(data=df[df['group_col']==group], x='value', label=group)\n",
    "plt.legend()\n",
    "plt.title('Cumulative Distribution by Group')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 7. Correlation Analysis (Beyond Pearson)\n",
    "```python\n",
    "# Comprehensive correlation analysis\n",
    "def advanced_correlation(df):\n",
    "    # Pearson (linear)\n",
    "    pearson = df.corr(method='pearson')\n",
    "    \n",
    "    # Spearman (monotonic)\n",
    "    spearman = df.corr(method='spearman')\n",
    "    \n",
    "    # Kendall (ordinal)\n",
    "    kendall = df.corr(method='kendall')\n",
    "    \n",
    "    # Distance correlation (nonlinear)\n",
    "    from dcor import distance_correlation\n",
    "    dist_corr = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            dist_corr.loc[col1, col2] = distance_correlation(df[col1], df[col2])\n",
    "    \n",
    "    # Visual comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16,12))\n",
    "    sns.heatmap(pearson, ax=axes[0,0], annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[0,0].set_title('Pearson Correlation')\n",
    "    \n",
    "    sns.heatmap(spearman, ax=axes[0,1], annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[0,1].set_title('Spearman Correlation')\n",
    "    \n",
    "    sns.heatmap(kendall, ax=axes[1,0], annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[1,0].set_title('Kendall Correlation')\n",
    "    \n",
    "    sns.heatmap(dist_corr.astype(float), ax=axes[1,1], annot=True, fmt=\".2f\", cmap='coolwarm', vmin=0, vmax=1)\n",
    "    axes[1,1].set_title('Distance Correlation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Partial correlation (controlling for other variables)\n",
    "import pingouin as pg\n",
    "partial_corr = pg.partial_corr(data=df, x='var1', y='var2', covar=['confounder1','confounder2'])\n",
    "print(partial_corr)\n",
    "\n",
    "# Nonlinear relationship detection\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "mi = mutual_info_regression(df[numeric_cols], df['target'])\n",
    "mi_series = pd.Series(mi, index=numeric_cols).sort_values(ascending=False)\n",
    "mi_series.plot(kind='bar')\n",
    "plt.title('Mutual Information with Target')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 8. Grouping & Aggregation (Advanced Patterns)\n",
    "```python\n",
    "# Multi-level aggregation\n",
    "agg_dict = {\n",
    "    'sales': ['sum', 'mean', 'median', 'std'],\n",
    "    'profit': lambda x: np.percentile(x, 90),\n",
    "    'quantity': 'count'\n",
    "}\n",
    "\n",
    "report = df.groupby(['region', 'product_category']).agg(agg_dict)\n",
    "report.columns = ['_'.join(col).strip() for col in report.columns.values]\n",
    "\n",
    "# Time-based resampling\n",
    "df.set_index('timestamp').resample('W').agg({\n",
    "    'value': ['mean', 'sum'],\n",
    "    'category': lambda x: x.mode()[0]\n",
    "})\n",
    "\n",
    "# Expanding window calculations\n",
    "df['rolling_avg'] = df.groupby('group')['value'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# Custom aggregation classes\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Aggregator(ABC):\n",
    "    @abstractmethod\n",
    "    def compute(self, data):\n",
    "        pass\n",
    "\n",
    "class OutlierResistantMean(Aggregator):\n",
    "    def compute(self, data):\n",
    "        return trim_mean(data, 0.1)\n",
    "\n",
    "df.groupby('group')['value'].agg(OutlierResistantMean().compute)\n",
    "\n",
    "# Conditional aggregation\n",
    "def weighted_avg(group):\n",
    "    return np.average(group['value'], weights=group['weight'])\n",
    "\n",
    "df.groupby('category').apply(weighted_avg)\n",
    "```\n",
    "\n",
    "### 9. Feature Engineering (Production Patterns)\n",
    "```python\n",
    "# Feature engineering pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TemporalFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['hour'] = X['timestamp'].dt.hour\n",
    "        X['is_weekend'] = X['timestamp'].dt.weekday >= 5\n",
    "        X['time_since_event'] = (X['timestamp'] - pd.Timestamp('2020-01-01')).dt.days\n",
    "        return X\n",
    "\n",
    "class TextFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['char_count'] = X['text'].str.len()\n",
    "        X['word_count'] = X['text'].str.split().str.len()\n",
    "        X['has_question'] = X['text'].str.contains('\\?').astype(int)\n",
    "        return X\n",
    "\n",
    "# Automated feature generation\n",
    "import featuretools as ft\n",
    "\n",
    "es = ft.EntitySet(id='data')\n",
    "es = es.entity_from_dataframe(entity_id='transactions', \n",
    "                            dataframe=df,\n",
    "                            index='id',\n",
    "                            time_index='timestamp')\n",
    "\n",
    "features, feature_defs = ft.dfs(entityset=es,\n",
    "                              target_entity='transactions',\n",
    "                              agg_primitives=['sum', 'mean', 'count'],\n",
    "                              trans_primitives=['month', 'weekday'])\n",
    "\n",
    "# Advanced encoding\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "interaction_features = poly.fit_transform(df[['feat1', 'feat2']])\n",
    "```\n",
    "\n",
    "### 10. Hypothesis Generation (Statistical Framework)\n",
    "```python\n",
    "# Hypothesis testing framework\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency\n",
    "\n",
    "def hypothesis_test(df, group_col, value_col, test_type='t-test'):\n",
    "    groups = df[group_col].unique()\n",
    "    if len(groups) != 2:\n",
    "        raise ValueError(\"Exactly 2 groups required\")\n",
    "    \n",
    "    group1 = df[df[group_col]==groups[0]][value_col]\n",
    "    group2 = df[df[group_col]==groups[1]][value_col]\n",
    "    \n",
    "    if test_type == 't-test':\n",
    "        stat, p = ttest_ind(group1, group2, equal_var=False)\n",
    "    elif test_type == 'mannwhitneyu':\n",
    "        stat, p = mannwhitneyu(group1, group2)\n",
    "    elif test_type == 'chi2':\n",
    "        cont_table = pd.crosstab(df[group_col], df[value_col])\n",
    "        stat, p, _, _ = chi2_contingency(cont_table)\n",
    "    \n",
    "    print(f\"{test_type.upper()} RESULTS:\")\n",
    "    print(f\"Statistic: {stat:.4f}\")\n",
    "    print(f\"p-value: {p:.4f}\")\n",
    "    print(f\"Significant at α=0.05: {p < 0.05}\")\n",
    "\n",
    "# Power analysis for sample size\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "analysis = TTestIndPower()\n",
    "required_n = analysis.solve_power(effect_size=0.5, alpha=0.05, power=0.8)\n",
    "print(f\"Required sample size per group: {required_n:.0f}\")\n",
    "\n",
    "# Multiple testing correction\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "pvals = [0.01, 0.04, 0.03, 0.21, 0.01]\n",
    "rejected, corrected_p, _, _ = multipletests(pvals, method='fdr_bh')\n",
    "print(f\"Corrected p-values: {corrected_p}\")\n",
    "\n",
    "# Bayesian A/B testing\n",
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    mu_A = pm.Normal('mu_A', mu=0, sd=10)\n",
    "    mu_B = pm.Normal('mu_B', mu=0, sd=10)\n",
    "    \n",
    "    # Likelihood\n",
    "    obs_A = pm.Normal('obs_A', mu=mu_A, sd=1, observed=group_A)\n",
    "    obs_B = pm.Normal('obs_B', mu=mu_B, sd=1, observed=group_B)\n",
    "    \n",
    "    # Difference\n",
    "    diff = pm.Deterministic('diff', mu_B - mu_A)\n",
    "    \n",
    "    # Inference\n",
    "    trace = pm.sample(2000, tune=1000)\n",
    "\n",
    "pm.plot_posterior(trace, var_names=['diff'])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Key Professional Practices:\n",
    "1. **Reproducibility**: Always set random seeds (`np.random.seed(42)`)\n",
    "2. **Modularity**: Create reusable EDA functions/classes\n",
    "3. **Documentation**: Use Jupyter notebooks with Markdown explanations\n",
    "4. **Performance**: Use `swifter` for parallelized operations on large data\n",
    "5. **Validation**: Implement unit tests for critical EDA functions\n",
    "6. **Visual Standardization**: Create style templates for consistent plots\n",
    "7. **Automation**: Build EDA pipelines using `sklearn.pipeline`\n",
    "8. **Versioning**: Track dataset and EDA code versions with DVC\n",
    "9. **Collaboration**: Export interactive reports with `plotly`/`ipywidgets`\n",
    "10. **Production Readiness**: Log all EDA steps for monitoring\n",
    "\n",
    "This comprehensive approach ensures your EDA is robust, reproducible, and production-ready while uncovering deep insights from your data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
