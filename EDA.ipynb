{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Missing Data, Categorical Data, Numeric Data, and Outliers in Data Analysis**\n",
    "\n",
    "## 1. Handling Missing Data\n",
    "Missing data can occur due to various reasons such as incomplete data collection, errors in data entry, or system failures. There are several strategies to handle missing data:\n",
    "\n",
    "### Techniques:\n",
    "- **Deletion Methods:**\n",
    "  - Listwise deletion: Removes entire rows with missing values.\n",
    "  - Pairwise deletion: Uses available data without removing entire rows.\n",
    "- **Imputation Methods:**\n",
    "  - Mean/Median/Mode imputation: Replaces missing values with statistical measures.\n",
    "  - Forward/Backward Fill: Uses previous or next available value to fill missing data.\n",
    "  - Interpolation: Estimates missing values based on trends.\n",
    "  - Machine Learning Imputation: Predicts missing values using models like KNN or regression.\n",
    "\n",
    "## 2. Handling Categorical Data\n",
    "Categorical data needs to be transformed into numerical format for machine learning models.\n",
    "\n",
    "### Encoding Techniques:\n",
    "- **Label Encoding:** Assigns a unique integer to each category.\n",
    "- **One-Hot Encoding:** Converts categorical variables into binary vectors.\n",
    "- **Ordinal Encoding:** Assigns ordered values to categories with a meaningful ranking.\n",
    "- **Target Encoding:** Replaces categories with the mean of the target variable.\n",
    "\n",
    "## 3. Handling Numeric Data\n",
    "Numeric data may have varying scales or distributions, requiring preprocessing.\n",
    "\n",
    "### Techniques:\n",
    "- **Standardization (Z-score normalization):** Transforms data to have a mean of 0 and standard deviation of 1.\n",
    "- **Min-Max Scaling:** Rescales data to a fixed range (0 to 1).\n",
    "- **Robust Scaling:** Uses median and interquartile range to handle outliers.\n",
    "\n",
    "## 4. Handling Outliers\n",
    "Outliers can distort analysis and predictions, so they must be detected and treated properly.\n",
    "\n",
    "### Detection Methods:\n",
    "- **Z-score Method:** Identifies data points beyond a threshold (e.g., |Z| > 3).\n",
    "- **Interquartile Range (IQR):** Flags values beyond 1.5 times the IQR.\n",
    "- **Boxplots & Visualization:** Uses plots to detect extreme values.\n",
    "- **Isolation Forest & DBSCAN:** Machine learning-based anomaly detection techniques.\n",
    "\n",
    "### Treatment Methods:\n",
    "- **Capping:** Limits extreme values to a fixed threshold.\n",
    "- **Transformation (Log, Square Root):** Reduces impact of outliers.\n",
    "- **Winsorization:** Replaces extreme values with nearest acceptable value.\n",
    "- **Removal:** Deletes outliers if justified by business needs.\n",
    "\n",
    "## Conclusion\n",
    "Proper handling of missing data, categorical variables, numeric transformations, and outliers is essential for building reliable and robust data models. Choosing the right technique depends on the dataset and analysis objectives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing Techniques\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', np.nan],\n",
    "    'Age': [25, np.nan, 35, 40, 29],\n",
    "    'Salary': [50000, 60000, np.nan, 80000, 70000],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "\n",
    "# Handling Missing Data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['Age'] = imputer.fit_transform(df[['Age']])\n",
    "df['Salary'] = imputer.fit_transform(df[['Salary']])\n",
    "df['Name'].fillna('Unknown', inplace=True)\n",
    "print(\"\\nAfter Handling Missing Data:\")\n",
    "print(df)\n",
    "\n",
    "# Encoding Categorical Data\n",
    "label_encoder = LabelEncoder()\n",
    "df['Department'] = label_encoder.fit_transform(df['Department'])\n",
    "print(\"\\nAfter Label Encoding:\")\n",
    "print(df)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "department_encoded = ohe.fit_transform(df[['Department']]).toarray()\n",
    "print(\"\\nAfter One-Hot Encoding:\")\n",
    "print(department_encoded)\n",
    "\n",
    "# Scaling Numeric Data\n",
    "scaler = StandardScaler()\n",
    "df[['Age', 'Salary']] = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(\"\\nAfter Standard Scaling:\")\n",
    "print(df)\n",
    "\n",
    "# Handling Outliers using Z-Score\n",
    "z_scores = np.abs(stats.zscore(df[['Age', 'Salary']]))\n",
    "df = df[(z_scores < 3).all(axis=1)]\n",
    "print(\"\\nAfter Handling Outliers:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing data\n",
    "It is important to deal with missing data before starting your analysis.\n",
    "\n",
    "One approach is to drop missing values if they account for a small proportion, typically five percent, of your data.\n",
    "\n",
    "Working with a dataset on plane ticket prices, stored as a pandas DataFrame called planes, you'll need to count the number of missing values across all columns, calculate five percent of all values, use this threshold to remove observations, and check how many missing values remain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of missing values in each column\n",
    "print(planes.isna().sum())\n",
    "\n",
    "\n",
    "# Find the five percent threshold\n",
    "threshold = len(planes) * 0.05\n",
    "\n",
    "\n",
    "# Create a filter\n",
    "cols_to_drop = planes.columns[planes.isna().sum() <= threshold]\n",
    "\n",
    "# Drop missing values for columns below the threshold\n",
    "planes.dropna(subset=cols_to_drop, inplace=True)\n",
    "print(planes.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies for remaining missing data\n",
    "The five percent rule has worked nicely for your planes dataset, eliminating missing values from nine out of 11 columns!\n",
    "\n",
    "Now, you need to decide what to do with the \"Additional_Info\" and \"Price\" columns, which are missing 300 and 368 values respectively.\n",
    "\n",
    "You'll first take a look at what \"Additional_Info\" contains, then visualize the price of plane tickets by different airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check the values of the Additional_Info column\n",
    "print(planes['Additional_Info'].value_counts())\n",
    "\n",
    "# Create a box plot of Price by Airline\n",
    "sns.boxplot(data=planes, x='Airline', y=\"Price\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "How should you deal with the missing values in \"Additional_Info\" and \"Price\"?\n",
    "Possible answers\n",
    "\n",
    "Remove the \"Additional_Info\" column and impute the median by \"Airline\" for missing values of \"Price\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing plane prices\n",
    "Now there's just one column with missing values left!\n",
    "\n",
    "You've removed the \"Additional_Info\" column from planes—the last step is to impute the missing data in the \"Price\" column of the dataset.\n",
    "\n",
    "As a reminder, you generated this boxplot, which suggested that imputing the median price based on the \"Airline\" is a solid approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate median plane ticket prices by Airline\n",
    "airline_prices = planes.groupby(\"Airline\")[\"Price\"].median()\n",
    "\n",
    "print(airline_prices)\n",
    "\n",
    "# Convert to a dictionary\n",
    "prices_dict = airline_prices.to_dict()\n",
    "\n",
    "# Map the dictionary to the missing values\n",
    "planes[\"Price\"] = planes[\"Price\"].fillna(planes[\"Airline\"].map(prices_dict))\n",
    "\n",
    "# Check for missing values\n",
    "print(planes.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the solution to understand how it addresses the exercise requirements:\n",
    "\n",
    "### Grouping and Calculating Median:\n",
    "\n",
    "`airline_prices = planes.groupby(\"Airline\")[\"Price\"].median()`\n",
    "This line groups the data in the planes DataFrame by the Airline column and calculates the median Price for each airline. The result is a Series where the index is the airline names and the values are the median prices. This step is crucial because it provides a typical price for each airline, which we will use to fill in the missing values.\n",
    "\n",
    "### Converting to a Dictionary:\n",
    "\n",
    "`prices_dict = airline_prices.to_dict()`\n",
    "Here, the Series airline_prices is converted into a dictionary called prices_dict. This dictionary maps each airline to its corresponding median price. This mapping is necessary for the next step, where we will use it to fill in missing values.\n",
    "\n",
    "### Mapping and Filling Missing Values:\n",
    "\n",
    "`planes[\"Price\"] = planes[\"Price\"].fillna(planes[\"Airline\"].map(prices_dict))`\n",
    "This line fills in the missing values in the Price column. It uses the fillna() method, which replaces NaN values with specified values. The map() function is applied to the Airline column, using prices_dict to replace each airline with its median price. This effectively imputes missing prices based on the typical price for each airline.\n",
    "\n",
    "### Checking for Remaining Missing Values:\n",
    "\n",
    "`print(planes.isna().sum())`\n",
    "Finally, this line checks for any remaining missing values in the DataFrame. The isna() method returns a DataFrame of the same shape as planes, with True for NaN values and False otherwise. The sum() method then counts the number of True values in each column, giving a summary of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the number of unique values\n",
    "You would like to practice some of the categorical data manipulation and analysis skills that you've just seen. To help identify which data could be reformatted to extract value, you are going to find out which non-numeric columns in the planes dataset have a large number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for object columns\n",
    "non_numeric = planes.select_dtypes(\"object\")\n",
    "\n",
    "# Loop through columns\n",
    "for column in non_numeric.columns:\n",
    "  \n",
    "  # Print the number of unique values\n",
    "  print(f\"Number of unique values in {column} column: \", non_numeric[column].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight duration categories\n",
    "As you saw, there are 362 unique values in the \"Duration\" column of planes. Calling planes[\"Duration\"].head(), we see the following values:\n",
    "\n",
    "- 0        19h\n",
    "- 1     5h 25m\n",
    "- 2     4h 45m\n",
    "- 3     2h 25m\n",
    "- 4    15h 30m\n",
    "\n",
    "Name: Duration, dtype: object\n",
    "\n",
    "Looks like this won't be simple to convert to numbers. However, you could categorize flights by duration and examine the frequency of different flight lengths!\n",
    "\n",
    "You'll create a \"Duration_Category\" column in the planes DataFrame. Before you can do this you'll need to create a list of the values you would like to insert into the DataFrame, followed by the existing values that these should be created from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of categories\n",
    "flight_categories = [\"Short-haul\", \"Medium\", \"Long-haul\"]\n",
    "\n",
    "# Create short_flights\n",
    "short_flights = \"^0h|^1h|^2h|^3h|^4h\"\n",
    "\n",
    "# Create medium_flights\n",
    "medium_flights = \"^5h|^6h|^7h|^8h|^9h\"\n",
    "\n",
    "# Create long_flights\n",
    "long_flights = \"10h|11h|12h|13h|14h|15h|16h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding duration categories\n",
    "Now that you've set up the categories and values you want to capture, it's time to build a new column to analyze the frequency of flights by duration!\n",
    "\n",
    "The variables flight_categories, short_flights, medium_flights, and long_flights that you previously created are available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conditions for values in flight_categories to be created\n",
    "conditions = [\n",
    "    (planes[\"Duration\"].str.contains(short_flights)),\n",
    "    (planes[\"Duration\"].str.contains(medium_flights)),\n",
    "    (planes[\"Duration\"].str.contains(long_flights))\n",
    "]\n",
    "\n",
    "# Apply the conditions list to the flight_categories\n",
    "planes[\"Duration_Category\"] = np.select(conditions, \n",
    "                                        flight_categories,\n",
    "                                        default=\"Extreme duration\")\n",
    "\n",
    "# Plot the counts of each category\n",
    "sns.countplot(data=planes, x=\"Duration_Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight duration\n",
    "You would like to analyze the duration of flights, but unfortunately, the \"Duration\" column in the planes DataFrame currently contains string values.\n",
    "\n",
    "You'll need to clean the column and convert it to the correct data type for analysis. seaborn has been imported as sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the column\n",
    "print(planes[\"Duration\"].head())\n",
    "\n",
    "# Remove the string character\n",
    "planes[\"Duration\"] = planes[\"Duration\"].str.replace(\"h\", \"\")\n",
    "\n",
    "# Convert to float data type\n",
    "planes[\"Duration\"] = planes[\"Duration\"].astype(float)\n",
    "\n",
    "# Plot a histogram\n",
    "planes['Duration'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding descriptive statistics\n",
    "Now \"Duration\" and \"Price\" both contain numeric values in the planes DataFrame, you would like to calculate summary statistics for them that are conditional on values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price standard deviation by Airline\n",
    "planes[\"airline_price_st_dev\"] = planes.groupby(\"Airline\")[\"Price\"].transform(lambda x: x.std())\n",
    "\n",
    "print(planes[[\"Airline\", \"airline_price_st_dev\"]].value_counts())\n",
    "\n",
    " # Median Duration by Airline\n",
    "planes[\"airline_median_duration\"] = planes.groupby(\"Airline\")[\"Duration\"].transform(lambda x: x.median())\n",
    "\n",
    "print(planes[[\"Airline\",\"airline_median_duration\"]].value_counts())\n",
    "\n",
    "# Mean Price by Destination\n",
    "planes[\"price_destination_mean\"] = planes.groupby(\"Destination\")[\"Price\"].transform(lambda x: x.mean())\n",
    "\n",
    "print(planes[[\"Destination\",\"price_destination_mean\"]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying outliers\n",
    "You've proven that you recognize what to do when presented with outliers, but can you identify them using visualizations?\n",
    "\n",
    "Try to figure out if there are outliers in the \"Price\" or \"Duration\" columns of the planes DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers\n",
    "While removing outliers isn't always the way to go, for your analysis, you've decided that you will only include flights where the \"Price\" is not an outlier.\n",
    "\n",
    "Therefore, you need to find the upper threshold and then use it to remove values above this from the planes DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 75th and 25th percentiles\n",
    "price_seventy_fifth = planes[\"Price\"].quantile(0.75)\n",
    "price_twenty_fifth = planes[\"Price\"].quantile(0.25)\n",
    "\n",
    "# Calculate iqr\n",
    "prices_iqr = price_seventy_fifth - price_twenty_fifth\n",
    "\n",
    "# Calculate the thresholds\n",
    "upper = price_seventy_fifth + (1.5 * prices_iqr)\n",
    "lower = price_twenty_fifth - (1.5 * prices_iqr)\n",
    "\n",
    "# Subset the data\n",
    "planes = planes[(planes[\"Price\"] > lower) & (planes[\"Price\"] < upper)]\n",
    "\n",
    "print(planes[\"Price\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing DateTime data\n",
    "You'll now work with the entire divorce dataset! The data describes Mexican marriages dissolved between 2000 and 2015. It contains marriage and divorce dates, education level, birthday, income for each partner, and marriage duration, as well as the number of children the couple had at the time of divorce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import divorce.csv, parsing the appropriate columns as dates in the import\n",
    "divorce = pd.read_csv('divorce.csv', parse_dates=[\"divorce_date\", \"dob_man\", \"dob_woman\", \"marriage_date\"])\n",
    "print(divorce.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating data type to DateTime\n",
    "Now, the divorce DataFrame has been loaded for you, but one column is stored as a string that should be DateTime data. Which one is it? Once you've identified the column, you'll update it so that you can explore it more closely in the next exercise.\n",
    "\n",
    "pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the marriage_date column to DateTime values\n",
    "divorce[\"marriage_date\"] = pd.to_datetime(divorce[\"marriage_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing relationships over time\n",
    "Now that your date data is saved as DateTime data, you can explore patterns over time! Does the year that a couple got married have a relationship with the number of children that the couple has at the time of divorce? Your task is to find out!\n",
    "\n",
    "The divorce DataFrame (with all dates formatted as DateTime data types) has been loaded for you. pandas has been loaded as pd, matplotlib.pyplot has been loaded as plt, and Seaborn has been loaded as sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the marriage_year column\n",
    "divorce[\"marriage_year\"] = divorce[\"marriage_date\"].dt.year\n",
    "\n",
    "# Create a line plot showing the average number of kids by year\n",
    "sns.lineplot(data=divorce, x=\"marriage_year\", y=\"num_kids\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a heatmap\n",
    "Which of the below statements is correct regarding the relationships between variables in the divorce DataFrame?\n",
    "\n",
    "The divorce DataFrame has been loaded for you so that you can explore it in the shell. pandas has been loaded as pd, matplotlib.pyplot has been loaded as plt, and Seaborn has been loaded as sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(divorce.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing variable relationships\n",
    "In the last exercise, you may have noticed that a longer marriage_duration is correlated with having more children, represented by the num_kids column. The correlation coefficient between the marriage_duration and num_kids variables is 0.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatterplot\n",
    "sns.scatterplot(data = divorce, x='marriage_duration',y = 'num_kids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing multiple variable relationships\n",
    "Seaborn's .pairplot() is excellent for understanding the relationships between several or all variables in a dataset by aggregating pairwise scatter plots in one visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot for income_woman and marriage_duration\n",
    "sns.pairplot(data=divorce, vars=['income_woman' , 'marriage_duration'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data in scatter plots\n",
    "In the video, we explored how men's education and age at marriage related to other variables in our dataset, the divorce DataFrame. Now, you'll take a look at how women's education and age at marriage relate to other variables!\n",
    "\n",
    "Your task is to create a scatter plot of each woman's age and income, layering in the categorical variable of education level for additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "sns.scatterplot(data=divorce,x='woman_age_marriage', y='income_woman', hue='education_woman'  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring with KDE plots\n",
    "Kernel Density Estimate (KDE) plots are a great alternative to histograms when you want to show multiple distributions in the same visual.\n",
    "\n",
    "Suppose you are interested in the relationship between marriage duration and the number of kids that a couple has. Since values in the num_kids column range only from one to five, you can plot the KDE for each value on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KDE plot\n",
    "# Update the KDE plot to show a cumulative distribution function\n",
    "sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\", cut=0, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for class imbalance\n",
    "Print the relative frequency of the \"Job_Category\" column from salaries DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the relative frequency of Job_Category\n",
    "print(salaries['Job_Category'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-tabulation can help identify how observations occur in combination.\n",
    "\n",
    "Using the salaries dataset, which has been imported as a pandas DataFrame, you'll perform cross-tabulation on multiple variables, including the use of aggregation, to see the relationship between \"Company_Size\" and other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate Company_Size and Experience\n",
    "print(pd.crosstab(salaries[\"Company_Size\"], salaries[\"Experience\"]))\n",
    "\n",
    "\n",
    "# Cross-tabulate Job_Category and Company_Size\n",
    "print(pd.crosstab(salaries[\"Job_Category\"], salaries[\"Company_Size\"]))\n",
    "\n",
    "\n",
    "# Cross-tabulate Job_Category and Company_Size\n",
    "print(pd.crosstab(salaries[\"Job_Category\"], salaries[\"Company_Size\"],\n",
    "            values=salaries[\"Salary_USD\"], aggfunc=\"mean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features for correlation\n",
    "In this exercise, you'll work with a version of the salaries dataset containing a new column called \"date_of_response\".\n",
    "\n",
    "The dataset has been read in as a pandas DataFrame, with \"date_of_response\" as a datetime data type.\n",
    "\n",
    "Your task is to extract datetime attributes from this column and then create a heat map to visualize the correlation coefficients between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the month of the response\n",
    "salaries[\"month\"] = salaries[\"date_of_response\"].dt.month\n",
    "\n",
    "# Extract the weekday of the response\n",
    "salaries[\"weekday\"] = salaries[\"date_of_response\"].dt.weekday\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(salaries.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating salary percentiles\n",
    "In the video, you saw that the conversion of numeric data into categories sometimes makes it easier to identify patterns.\n",
    "\n",
    "Your task is to convert the \"Salary_USD\" column into categories based on its percentiles. First, you need to find the percentiles and store them as variables.\n",
    "\n",
    "pandas has been imported as pd and the salaries dataset read in as DataFrame called salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 25th percentile\n",
    "twenty_fifth = salaries[\"Salary_USD\"].quantile(0.25)\n",
    "\n",
    "# Save the median\n",
    "salaries_median = salaries[\"Salary_USD\"].median()\n",
    "\n",
    "# Gather the 75th percentile\n",
    "seventy_fifth = salaries[\"Salary_USD\"].quantile(0.75)\n",
    "print(twenty_fifth, salaries_median, seventy_fifth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing salaries\n",
    "Now it's time to make a new category! You'll use the variables twenty_fifth, salaries_median, and seventy_fifth, that you created in the previous exercise, to split salaries into different labels.\n",
    "\n",
    "The result will be a new column called \"salary_level\", which you'll incorporate into a visualization to analyze survey respondents' salary and at companies of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create salary labels\n",
    "salary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]\n",
    "\n",
    "# Create the salary ranges list\n",
    "salary_ranges = [0, twenty_fifth, salaries_median, seventy_fifth, salaries[\"Salary_USD\"].max()]\n",
    "\n",
    "# Create salary_level\n",
    "salaries[\"salary_level\"] = pd.cut(salaries[\"Salary_USD\"],\n",
    "                                  bins=salary_ranges,\n",
    "                                  labels=salary_labels)\n",
    "\n",
    "# Plot the count of salary levels at companies of different sizes\n",
    "sns.countplot(data=salaries, x=\"Company_Size\", hue=\"salary_level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing salaries\n",
    "Exploratory data analysis is a crucial step in generating hypotheses!\n",
    "\n",
    "You've had an idea you'd like to explore—do data professionals get paid more in the USA than they do in Great Britain?\n",
    "\n",
    "You'll need to subset the data by \"Employee_Location\" and produce a plot displaying the average salary between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for employees in the US or GB\n",
    "usa_and_gb = salaries[salaries[\"Employee_Location\"].isin([\"US\", \"GB\"])]\n",
    "\n",
    "# Create a barplot of salaries by location\n",
    "sns.barplot(data=usa_and_gb, x=\"Employee_Location\", y=\"Salary_USD\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
